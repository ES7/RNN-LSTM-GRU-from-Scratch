{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44962cd9-2420-47a4-b169-7624d8b34754",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d2e5fbb-a557-416a-8781-82d5fe229bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d126568d-1c3a-498e-9775-e03cdc4cee4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sayed\\AppData\\Local\\Temp\\ipykernel_29192\\4293216595.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU available: True\")\n",
    "else:\n",
    "    print(\"GPU available: False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a1abc8-e88d-4dc7-83dd-d1d119d6f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output\n",
    "        self.bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "        self.by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "    def forward(self, inputs, h_prev):\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = np.zeros((self.input_size, 1))\n",
    "            xs[t][inputs[t]] = 1  # One-hot encoding of input\n",
    "            hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t - 1]) + self.bh)  # Hidden state\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by  # Unnormalized log probabilities for output\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # Softmax to get probabilities\n",
    "        return xs, hs, ps\n",
    "\n",
    "    def backward(self, inputs, targets, xs, hs, ps):\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets] -= 1  # Backprop into y, targets is now a single value\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext  # Backprop into h\n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh  # Backprop through tanh nonlinearity\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t - 1].T)\n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)  # Clip gradients to prevent exploding gradients\n",
    "        return dWxh, dWhh, dWhy, dbh, dby\n",
    "\n",
    "    def train(self, inputs, targets, learning_rate=0.1, num_epochs=1000):\n",
    "        for epoch in range(num_epochs):\n",
    "            h_prev = np.zeros((self.hidden_size, 1))  # Initialize hidden state at the beginning of each epoch\n",
    "            loss = 0\n",
    "            xs, hs, ps = self.forward(inputs, h_prev)\n",
    "            dWxh, dWhh, dWhy, dbh, dby = self.backward(inputs, targets, xs, hs, ps)\n",
    "            self.Wxh -= learning_rate * dWxh\n",
    "            self.Whh -= learning_rate * dWhh\n",
    "            self.Why -= learning_rate * dWhy\n",
    "            self.bh -= learning_rate * dbh\n",
    "            self.by -= learning_rate * dby\n",
    "            loss += -np.sum([np.log(ps[t][targets[t], 0]) for t in range(len(inputs))])\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def sample(self, seed_index, n):\n",
    "        x = np.zeros((self.input_size, 1))\n",
    "        x[seed_index] = 1\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        indices = []\n",
    "        for _ in range(n):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            idx = np.random.choice(range(self.output_size), p=p.ravel())\n",
    "            x = np.zeros((self.input_size, 1))\n",
    "            x[idx] = 1\n",
    "            indices.append(idx)\n",
    "        return indices\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "        xs, hs, ps = self.forward(inputs, h_prev)\n",
    "        predictions = [np.argmax(ps[t]) for t in range(len(inputs))]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf7d04f-3fc4-4d32-b2e0-35a580582170",
   "metadata": {},
   "source": [
    "### Class initialization Parameters\n",
    "- **input_size:** The dimensionality of the input data. For example, if the input data is one-hot encoded vectors of size 100, then input_size would be 100.\n",
    "- **hidden_size:** The number of units in the hidden layer. This determines the capacity of the network to capture information from the input sequence.\n",
    "- **output_size:** The dimensionality of the output data. For example, if we are predicting a probability distribution over 10 classes, then output_size would be 10.\n",
    "\n",
    "### Weights and Biases\n",
    "- **Wxh:** The weight matrix for the connections between the input layer and the hidden layer. It has dimensions (hidden_size, input_size).\n",
    "- **Whh:** The weight matrix for the connections within the hidden layer (i.e., from the previous hidden state to the current hidden state). It has dimensions (hidden_size, hidden_size).\n",
    "- **Why:** The weight matrix for the connections between the hidden layer and the output layer. It has dimensions (output_size, hidden_size).\n",
    "- **bh:** The bias vector for the hidden layer. It has dimensions (hidden_size, 1).\n",
    "- **by:** The bias vector for the output layer. It has dimensions (output_size, 1).\n",
    "\n",
    "### Forward Pass Variables\n",
    "- **xs:** A dictionary storing the input vectors at each time step.\n",
    "- **hs:** A dictionary storing the hidden state vectors at each time step.\n",
    "- **ys:** A dictionary storing the unnormalized log probabilities (before applying softmax) for the outputs at each time step.\n",
    "- **ps:** A dictionary storing the probabilities (after applying softmax) for the outputs at each time step.\n",
    "\n",
    "### Backward Pass Variables\n",
    "- **dWxh:** The gradient of the loss with respect to the weight matrix Wxh.\n",
    "- **dWhh:** The gradient of the loss with respect to the weight matrix Whh.\n",
    "- **dWhy:** The gradient of the loss with respect to the weight matrix Why.\n",
    "- **dbh:** The gradient of the loss with respect to the bias vector bh.\n",
    "- **dby:** The gradient of the loss with respect to the bias vector by.\n",
    "- **dhnext:** The gradient of the loss with respect to the hidden state from the next time step, used in the backward pass to accumulate gradients through time.\n",
    "\n",
    "### Training Parameters\n",
    "- **inputs:** A list of integers representing the input sequence (e.g., one-hot encoded indices).\n",
    "- **targets:** A list of integers representing the target sequence for training.\n",
    "- **learning_rate:** The step size for updating the weights during training.\n",
    "- **num_epochs:** The number of training iterations over the entire dataset.\n",
    "\n",
    "### Sample Method Variables\n",
    "- **seed_index:** The starting index for generating a sequence.\n",
    "- **n:** The length of the sequence to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a07116f-cae7-48cd-8834-ca7f52b5f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Load the IMDB dataset, keeping only the top 5000 words and using sequences of up to 500 words\n",
    "top_words = 5000\n",
    "max_review_length = 500\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# Pad sequences to ensure they are all the same length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74c1ad-25bb-430f-a68d-5bf65f7f45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_on_imdb(rnn, x_train, y_train, num_epochs=10, learning_rate=0.1):\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = 0\n",
    "        for i in range(len(x_train)):\n",
    "            inputs = x_train[i]\n",
    "            target = y_train[i]\n",
    "\n",
    "            # Forward pass\n",
    "            xs, hs, ps = rnn.forward(inputs, np.zeros((rnn.hidden_size, 1)))\n",
    "\n",
    "            # Backward pass\n",
    "            targets = target  # Single target value\n",
    "            dWxh, dWhh, dWhy, dbh, dby = rnn.backward(inputs, targets, xs, hs, ps)\n",
    "\n",
    "            # Update weights and biases\n",
    "            rnn.Wxh -= learning_rate * dWxh\n",
    "            rnn.Whh -= learning_rate * dWhh\n",
    "            rnn.Why -= learning_rate * dWhy\n",
    "            rnn.bh -= learning_rate * dbh\n",
    "            rnn.by -= learning_rate * dby\n",
    "\n",
    "            # Calculate loss\n",
    "            loss += -np.sum(np.log(ps[len(inputs) - 1][targets, 0]))\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Initialize the RNN\n",
    "input_size = top_words\n",
    "hidden_size = 100\n",
    "output_size = 2  # Binary classification (2 classes)\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the RNN on IMDB dataset\n",
    "train_rnn_on_imdb(rnn, x_train, y_train, num_epochs=10, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa46db5-6f61-45dd-8c7c-c7ad874fab65",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b2a0a-60e5-426c-bfe1-769a809dfcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(rnn, x_test, y_test):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(x_test)\n",
    "\n",
    "    for i in range(total_predictions):\n",
    "        inputs = x_test[i]\n",
    "        target = y_test[i]\n",
    "        predictions = rnn.predict(inputs)\n",
    "        if predictions[-1] == target:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Assuming x_test and y_test are your test datasets\n",
    "accuracy = evaluate_model(rnn, x_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3b419-7157-45cb-beef-11fc43a78f29",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6050d92d-2a22-4655-95f5-0f35cbf3c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(rnn, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(rnn, file)\n",
    "\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "save_model(rnn, 'rnn_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae673c0-c3a4-4b5a-ae73-84f64f151c9e",
   "metadata": {},
   "source": [
    "# BiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb45e366-16b8-41ec-8f98-9568e804b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.forward_rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
    "        self.backward_rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        forward_outputs, _ = self.forward_rnn.forward(inputs)\n",
    "        backward_outputs, _ = self.backward_rnn.forward(inputs[::-1])  # Reverse the input sequence\n",
    "\n",
    "        # Concatenate the forward and backward outputs\n",
    "        outputs = np.concatenate((forward_outputs, backward_outputs[::-1]), axis=1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def train(self, inputs, targets, learning_rate=0.1, num_epochs=1000):\n",
    "        for epoch in range(num_epochs):\n",
    "            forward_loss = self.forward_rnn.train(inputs, targets, learning_rate)\n",
    "            backward_loss = self.backward_rnn.train(inputs[::-1], targets[::-1], learning_rate)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Forward Loss: {forward_loss}, Backward Loss: {backward_loss}\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        forward_predictions = self.forward_rnn.predict(inputs)\n",
    "        backward_predictions = self.backward_rnn.predict(inputs[::-1])  # Reverse the input sequence\n",
    "\n",
    "        # Concatenate the forward and backward predictions\n",
    "        predictions = np.concatenate((forward_predictions, backward_predictions[::-1]), axis=1)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c65a69-009f-426f-9467-0dc2c795578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000 \n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "input_size = max_features\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10\n",
    "\n",
    "bi_rnn = BiRNN(input_size, hidden_size, output_size)\n",
    "bi_rnn.train(x_train, y_train, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df899383-008c-4241-b5fd-0fcb1df56235",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73699adb-8975-4f84-b2c7-b3752410a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_model(bi_rnn, x_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80979d0-2b89-489a-994c-542bbb253ac1",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038aacff-c73b-47f5-ad1b-c7159870b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(bo_rnn, 'lstm_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf813f5-96f0-4970-b59a-40eabe5501da",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee383c-1773-4f0f-a28b-14c4e7a47bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.Wf = np.random.randn(hidden_size, input_size + hidden_size) * 0.01  # Forget gate\n",
    "        self.Wi = np.random.randn(hidden_size, input_size + hidden_size) * 0.01  # Input gate\n",
    "        self.Wc = np.random.randn(hidden_size, input_size + hidden_size) * 0.01  # Candidate value\n",
    "        self.Wo = np.random.randn(hidden_size, input_size + hidden_size) * 0.01  # Output gate\n",
    "        self.Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output\n",
    "\n",
    "        self.bf = np.zeros((hidden_size, 1))  # Forget gate bias\n",
    "        self.bi = np.zeros((hidden_size, 1))  # Input gate bias\n",
    "        self.bc = np.zeros((hidden_size, 1))  # Candidate value bias\n",
    "        self.bo = np.zeros((hidden_size, 1))  # Output gate bias\n",
    "        self.by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "    def forward(self, inputs, h_prev, c_prev):\n",
    "        xs, hs, cs, ys, ps = {}, {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "        cs[-1] = np.copy(c_prev)\n",
    "\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = np.zeros((self.input_size, 1))\n",
    "            xs[t][inputs[t]] = 1  # One-hot encoding of input\n",
    "\n",
    "            concat = np.vstack((hs[t-1], xs[t]))\n",
    "\n",
    "            ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)  # Forget gate\n",
    "            it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)  # Input gate\n",
    "            cct = np.tanh(np.dot(self.Wc, concat) + self.bc)  # Candidate value\n",
    "            ct = ft * cs[t-1] + it * cct  # New cell state\n",
    "            ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)  # Output gate\n",
    "            ht = ot * np.tanh(ct)  # New hidden state\n",
    "\n",
    "            ys[t] = np.dot(self.Wy, ht) + self.by  # Unnormalized log probabilities for output\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # Softmax to get probabilities\n",
    "\n",
    "            hs[t] = ht\n",
    "            cs[t] = ct\n",
    "\n",
    "        return xs, hs, cs, ps\n",
    "\n",
    "    def backward(self, inputs, targets, xs, hs, cs, ps):\n",
    "        dWf, dWi, dWc, dWo, dWy = np.zeros_like(self.Wf), np.zeros_like(self.Wi), np.zeros_like(self.Wc), np.zeros_like(self.Wo), np.zeros_like(self.Wy)\n",
    "        dbf, dbi, dbc, dbo, dby = np.zeros_like(self.bf), np.zeros_like(self.bi), np.zeros_like(self.bc), np.zeros_like(self.bo), np.zeros_like(self.by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        dcnext = np.zeros_like(cs[0])\n",
    "\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1  # Backprop into y\n",
    "\n",
    "            dWy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            dh = np.dot(self.Wy.T, dy) + dhnext  # Backprop into h\n",
    "\n",
    "            do = dh * np.tanh(cs[t])\n",
    "            do = self.sigmoid_derivative(do, hs[t])\n",
    "\n",
    "            dc = dh * cs[t] * (1 - np.tanh(cs[t])**2) + dcnext\n",
    "            dcct = dc * cs[t-1] * (1 - cs[t-1]**2)\n",
    "            dcct = self.tanh_derivative(dcct, cs[t-1])\n",
    "\n",
    "            di = dc * cs[t-1]\n",
    "            di = self.sigmoid_derivative(di, cs[t-1])\n",
    "\n",
    "            df = dc * cs[t-1]\n",
    "            df = self.sigmoid_derivative(df, cs[t-1])\n",
    "\n",
    "            dconcat = np.dot(self.Wf.T, df) + np.dot(self.Wi.T, di) + np.dot(self.Wc.T, dcct) + np.dot(self.Wo.T, do)\n",
    "            dconcat = dconcat[:self.hidden_size, :] + dconcat[self.hidden_size:, :]\n",
    "\n",
    "            dWf += np.dot(df, dconcat.T)\n",
    "            dbf += df\n",
    "            dWi += np.dot(di, dconcat.T)\n",
    "            dbi += di\n",
    "            dWc += np.dot(dcct, dconcat.T)\n",
    "            dbc += dcct\n",
    "            dWo += np.dot(do, dconcat.T)\n",
    "            dbo += do\n",
    "\n",
    "            dhnext = dconcat[:self.hidden_size, :]\n",
    "            dcnext = dc * cs[t-1]\n",
    "\n",
    "        for dparam in [dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)  # Clip gradients to prevent exploding gradients\n",
    "\n",
    "        return dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, sigmoid, x):\n",
    "        return sigmoid * (1 - sigmoid)\n",
    "\n",
    "    def tanh_derivative(self, tanh, x):\n",
    "        return 1 - tanh**2\n",
    "\n",
    "    def train(self, inputs, targets, learning_rate=0.1, num_epochs=1000):\n",
    "        for epoch in range(num_epochs):\n",
    "            h_prev = np.zeros((self.hidden_size, 1))\n",
    "            c_prev = np.zeros((self.hidden_size, 1))\n",
    "            loss = 0\n",
    "\n",
    "            xs, hs, cs, ps = self.forward(inputs, h_prev, c_prev)\n",
    "            dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby = self.backward(inputs, targets, xs, hs, cs, ps)\n",
    "\n",
    "            self.Wf -= learning_rate * dWf\n",
    "            self.Wi -= learning_rate * dWi\n",
    "            self.Wc -= learning_rate * dWc\n",
    "            self.Wo -= learning_rate * dWo\n",
    "            self.Wy -= learning_rate * dWy\n",
    "            self.bf -= learning_rate * dbf\n",
    "            self.bi -= learning_rate * dbi\n",
    "            self.bc -= learning_rate * dbc\n",
    "            self.bo -= learning_rate * dbo\n",
    "            self.by -= learning_rate * dby\n",
    "\n",
    "            loss += -np.sum([np.log(ps[t][targets[t], 0]) for t in range(len(inputs))])\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "        c_prev = np.zeros((self.hidden_size, 1))\n",
    "        xs, hs, cs, ps = self.forward(inputs, h_prev, c_prev)\n",
    "        predictions = [np.argmax(ps[t]) for t in range(len(inputs))]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6fab57-5fa2-4eb8-8539-ddc77d6be192",
   "metadata": {},
   "source": [
    "- **input_size:** The size of the input vocabulary or the number of unique words in the input sequences. This determines the size of the input layer of the LSTM.\n",
    "- **hidden_size:** The number of hidden units in the LSTM cell. This parameter determines the complexity and capacity of the LSTM to learn patterns in the input sequences.\n",
    "- **output_size:** The size of the output layer, which is typically the number of classes in a classification task. For binary classification tasks like sentiment analysis, output_size is set to 2 (positive or negative).\n",
    "- **Wf, Wi, Wc, Wo, Wy:** Weight matrices for the forget gate, input gate, candidate value, output gate, and output layer, respectively. These matrices are learned during training to capture patterns in the input sequences.\n",
    "- **bf, bi, bc, bo, by:** Bias vectors for the forget gate, input gate, candidate value, output gate, and output layer, respectively. These biases help the LSTM model learn the appropriate transformations for the input data.\n",
    "- **h_prev:** The previous hidden state of the LSTM cell, initialized as zeros. This state is used to initialize the hidden state at the beginning of each input sequence.\n",
    "- **c_prev:** The previous cell state of the LSTM cell, initialized as zeros. This state is used to initialize the cell state at the beginning of each input sequence.\n",
    "- **xs, hs, cs, ys, ps:** Dictionaries to store the input vectors, hidden states, cell states, unnormalized log probabilities, and softmax probabilities for each time step in the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf1406-6fb8-49cb-8377-dcdbc72b8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM model\n",
    "input_size = max_features\n",
    "hidden_size = 128\n",
    "output_size = 2  # Binary classification (positive or negative review)\n",
    "\n",
    "lstm = LSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "# Training on a single sample for simplicity\n",
    "inputs = x_train[0]\n",
    "targets = [y_train[0]] * len(inputs)  # Repeat target for each time step\n",
    "\n",
    "lstm.train(inputs, targets, learning_rate=0.1, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f62b8e-655e-4609-821b-9eb58c79765e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fad0e6-aee8-4f3b-ae5d-53003c59c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_model(lstm, x_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f123d14-64bb-4a42-a035-ce9a416dd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(lstm, 'lstm_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f3b78-28bf-4e4e-9eb2-1f3fb69ec10a",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b99797-58ae-407e-ac7c-b4a7649ee1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Weight matrices\n",
    "        self.Wz = np.random.randn(hidden_size, input_size + hidden_size) * 0.01  # Update gate weights\n",
    "        self.Wr = np.random.randn(hidden_size, input_size + hidden_size) * 0.01  # Reset gate weights\n",
    "        self.Wh = np.random.randn(hidden_size, input_size + hidden_size) * 0.01  # Output weights\n",
    "        self.Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
    "\n",
    "        # Biases\n",
    "        self.bz = np.zeros((hidden_size, 1))  # Update gate bias\n",
    "        self.br = np.zeros((hidden_size, 1))  # Reset gate bias\n",
    "        self.bh = np.zeros((hidden_size, 1))  # Output bias\n",
    "        self.by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "    def forward(self, inputs, h_prev):\n",
    "        xs, zs, rs, hs, ys, ps = {}, {}, {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = np.zeros((self.input_size, 1))\n",
    "            xs[t][inputs[t]] = 1  # One-hot encoding of input\n",
    "\n",
    "            concat = np.vstack((hs[t-1], xs[t]))\n",
    "\n",
    "            # Update gate\n",
    "            zs[t] = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "            # Reset gate\n",
    "            rs[t] = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "            # Hidden state\n",
    "            hs[t] = zs[t] * hs[t-1] + (1 - zs[t]) * np.tanh(np.dot(self.Wh, np.vstack((rs[t] * hs[t-1], xs[t]))) + self.bh)\n",
    "            # Output\n",
    "            ys[t] = np.dot(self.Wy, hs[t]) + self.by\n",
    "            # Softmax to get probabilities\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "\n",
    "        return xs, zs, rs, hs, ps\n",
    "\n",
    "    def backward(self, inputs, targets, xs, zs, rs, hs, ps):\n",
    "        dWz, dWr, dWh, dWy = np.zeros_like(self.Wz), np.zeros_like(self.Wr), np.zeros_like(self.Wh), np.zeros_like(self.Wy)\n",
    "        dbz, dbr, dbh, dby = np.zeros_like(self.bz), np.zeros_like(self.br), np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1  # Backprop into y\n",
    "\n",
    "            dWy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            dh = np.dot(self.Wy.T, dy) + dhnext  # Backprop into h\n",
    "\n",
    "            dhraw = (1 - hs[t] ** 2) * dh  # Backprop through tanh\n",
    "\n",
    "            dh_prev_reset = np.dot(self.Wh.T, dhraw)\n",
    "\n",
    "            # Backprop through update gate\n",
    "            dz = hs[t-1] - hs[t]\n",
    "            dz = self.sigmoid_derivative(zs[t], dz)\n",
    "            dWz += np.dot(dz, np.vstack((hs[t-1], xs[t])).T)\n",
    "            dbz += dz\n",
    "\n",
    "            # Backprop through reset gate\n",
    "            dr = np.dot(self.Wh[:, :self.hidden_size].T, dhraw)\n",
    "            dr = self.sigmoid_derivative(rs[t], dr)\n",
    "            dWr += np.dot(dr, np.vstack((hs[t-1], xs[t])).T)\n",
    "            dbr += dr\n",
    "\n",
    "            # Backprop through hidden state\n",
    "            dWh += np.dot(dhraw, np.vstack((rs[t] * hs[t-1], xs[t])).T)\n",
    "            dbh += dhraw\n",
    "\n",
    "            dhnext = np.dot(self.Wh[:, :self.hidden_size].T, dhraw)\n",
    "\n",
    "        for dparam in [dWz, dWr, dWh, dWy, dbz, dbr, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)  # Clip gradients to prevent exploding gradients\n",
    "\n",
    "        return dWz, dWr, dWh, dWy, dbz, dbr, dbh, dby\n",
    "\n",
    "    def train(self, inputs, targets, learning_rate=0.1, num_epochs=1000):\n",
    "        for epoch in range(num_epochs):\n",
    "            h_prev = np.zeros((self.hidden_size, 1))\n",
    "            loss = 0\n",
    "\n",
    "            xs, zs, rs, hs, ps = self.forward(inputs, h_prev)\n",
    "            dWz, dWr, dWh, dWy, dbz, dbr, dbh, dby = self.backward(inputs, targets, xs, zs, rs, hs, ps)\n",
    "\n",
    "            self.Wz -= learning_rate * dWz\n",
    "            self.Wr -= learning_rate * dWr\n",
    "            self.Wh -= learning_rate * dWh\n",
    "            self.Wy -= learning_rate * dWy\n",
    "            self.bz -= learning_rate * dbz\n",
    "            self.br -= learning_rate * dbr\n",
    "            self.bh -= learning_rate * dbh\n",
    "            self.by -= learning_rate * dby\n",
    "\n",
    "            loss += -np.sum([np.log(ps[t][targets[t], 0]) for t in range(len(inputs))])\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "        xs, zs, rs, hs, ps = self.forward(inputs, h_prev)\n",
    "        predictions = [np.argmax(ps[t]) for t in range(len(inputs))]\n",
    "        return predictions\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, sigmoid, x):\n",
    "        return sigmoid * (1 - sigmoid)\n",
    "\n",
    "    def tanh_derivative(self, tanh, x):\n",
    "        return 1 - tanh ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85cbf80-418a-4927-af5c-171896ca6961",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_epochs = 5\n",
    "\n",
    "gru = GRU(input_size=max_features, hidden_size=hidden_size, output_size=output_size)\n",
    "gru.train(input_train, y_train, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7360c6e-45bb-412b-b91a-9287f294dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_model(gru, x_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf8a252-b95c-4aec-b191-bf5eff174552",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(gru, 'gru_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d6b3ae-781c-49b5-8e6a-be295a2272cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
