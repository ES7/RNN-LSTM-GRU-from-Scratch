{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c64e6cc-d728-4e99-9675-e141992f3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1f698-caf2-4ecf-9aea-3380abdea6de",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd05a348-fd70-4f80-be96-9cadd79fefcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = r'Flickr8k_Dataset//Images'\n",
    "CAPTION_FILE = r'Flickr8k_Dataset//captions.txt'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 32\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_EPOCHS = 2\n",
    "MAX_LEN = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7f916-1570-49c0-909a-1f4cf3084348",
   "metadata": {},
   "source": [
    "* `BATCH_SIZE`: Number of samples per batch fed to the model during training.\n",
    "* `EMBED_SIZE`: Dimensionality of the word embedding vectors or image feature vectors.\n",
    "* `HIDDEN_SIZE`: Number of hidden units in the RNN (LSTM/GRU) decoder.\n",
    "* `NUM_EPOCHS`: Number of times the training loop goes over the entire dataset.\n",
    "* `MAX_LEN`: Maximum length of the caption (number of words) generated or considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a101a-4b95-48fe-89f2-d93af3219a4d",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3812669-6556-4b86-b724-cadb5d50b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}  # index-to-string\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}              # string-to-index\n",
    "        self.freq = Counter()\n",
    "\n",
    "    def build_vocab(self, sentence_list):\n",
    "        for sentence in sentence_list:\n",
    "            for word in nltk.tokenize.word_tokenize(sentence.lower()):\n",
    "                self.freq[word] += 1\n",
    "\n",
    "        idx = len(self.itos)  # start after the special tokens\n",
    "        for word, count in self.freq.items():\n",
    "            if count >= self.freq_threshold:\n",
    "                if word not in self.stoi:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = nltk.tokenize.word_tokenize(text.lower())\n",
    "        return [\n",
    "            self.stoi.get(token, self.stoi[\"<UNK>\"])\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f377dc-d032-41fb-bbbc-38873c395636",
   "metadata": {},
   "source": [
    "* `stoi (string to index)`: Dictionary mapping words → unique integer indices.\n",
    "* `itos (index to string)`: Dictionary mapping indices → corresponding words.\n",
    "* `freq`: Keeps track of word frequency (how often each word appears).\n",
    "\n",
    "The **`__init__`** Function:\n",
    "- `freq_threshold`: Minimum frequency a word must have to be included in the vocabulary.\n",
    "- `itos`: Predefined special tokens with their fixed indices: <br>\n",
    "  - `<PAD>` (padding token) → index 0\n",
    "  - `<SOS>` (start of sentence token) → index 1\n",
    "  - `<EOS>` (end of sentence token) → index 2\n",
    "  - `<UNK>` (unknown token for rare/unseen words) → index 3\n",
    "- `stoi`: Reverse mapping from the special tokens (itos) for easy lookup.\n",
    "- `freq`: A Python Counter object that counts how many times each word appears.\n",
    "\n",
    "The **`build_vocab`** Function:\n",
    "- Input: sentence_list is a list of sentences (captions). This loop tokenizes every sentence into words using `nltk.tokenize.word_tokenize` (splits sentences into tokens).\n",
    "- Converts each word to lowercase for consistency.\n",
    "- Updates the frequency counter for each word across the entire dataset.\n",
    "\n",
    "- After counting frequencies, this builds the vocabulary:\n",
    "  - idx starts after the special tokens (so index 4 onwards).\n",
    "  - Iterates over each word and its count in the frequency dictionary.\n",
    "  - Includes only words with frequency >= freq_threshold.\n",
    "  - Adds each word to the mappings:\n",
    "    - stoi[word] = idx → word to index\n",
    "    - itos[idx] = word → index to word\n",
    "  - Increments idx for the next word.\n",
    "\n",
    "This ensures the vocabulary contains only common enough words, ignoring very rare ones.\n",
    "\n",
    "The **`numericalize`** Function:\n",
    "- Converts any input text string to a list of integers representing the tokens.\n",
    "- Tokenizes the input text. For each token:\n",
    "   - Looks up its index in stoi.\n",
    "   - If the word is not in the vocabulary, replaces it with the index of <UNK>.\n",
    "- Returns the list of indices for use as input to the model.\n",
    "\n",
    "The **`__len__`** Function returns the size of the vocabulary, including special tokens. Allows using `len(vocab)` to get the total number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b9031-3fbe-4751-a019-b20546dec621",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b214e2-f9b7-4e75-bbc3-d688d6790dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root, captions_file, transform=None, freq_threshold=5):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.imgs, self.captions = self.load_captions(captions_file)\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.captions)\n",
    "\n",
    "    def load_captions(self, caption_file):\n",
    "        with open(caption_file, 'r') as f:\n",
    "            lines = f.readlines()[1:]  # skip header if there's one\n",
    "    \n",
    "        print(\"Sample caption lines:\")\n",
    "        for i in range(5):\n",
    "            print(repr(lines[i]))\n",
    "    \n",
    "        imgs = []\n",
    "        captions = []\n",
    "    \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if ',' not in line:\n",
    "                continue\n",
    "            img, caption = line.split(',', 1)  # split only on first comma\n",
    "            imgs.append(img.strip())\n",
    "            captions.append(caption.strip())\n",
    "    \n",
    "        print(f\"Loaded {len(imgs)} image-caption pairs.\")\n",
    "        return imgs, captions\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.captions[idx]\n",
    "        img_id = self.imgs[idx]\n",
    "        img_path = os.path.join(self.root, img_id)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return image, torch.tensor(numericalized_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980912ee-6fe0-4120-839f-456fda8e0111",
   "metadata": {},
   "source": [
    "The **`__init__`** Function:\n",
    "- `root`: Path to the folder containing the images.\n",
    "- `captions_file`: Path to the text file that contains image file names and captions.\n",
    "- `transform`: Optional torchvision transforms to apply on images (like resizing, normalization, etc.).\n",
    "- `freq_threshold`: Minimum frequency threshold for including words in the vocabulary.\n",
    "\n",
    "It loads all image filenames and captions using `self.load_captions()`; Instantiates a Vocabulary object with the frequency threshold and Builds the vocabulary from the entire list of captions.\n",
    "\n",
    "The **`load_captions`** Function:\n",
    "- Opens the captions file and reads all lines except the first (assuming it's a header).\n",
    "- Prints the first 5 lines (for debugging/verification).\n",
    "- Iterates over every line:\n",
    "   - Removes whitespace.\n",
    "   - Checks if the line contains a comma (to prevent malformed lines).\n",
    "   - Splits into img and caption on the first comma only, because captions can contain commas.\n",
    "   - Adds the image filename and caption to respective lists.\n",
    "- Returns two lists: one of image filenames and one of captions.\n",
    "\n",
    "The **`__len__`** Function returns the total number of data points (image-caption pairs). Enables using `len(dataset)`.\n",
    "\n",
    "The **`__getitem__`** Function:\n",
    "- For a given index retrieves the caption and image filename.\n",
    "- Joins the root directory with the image filename to get the full path.\n",
    "- Opens the image using PIL and converts it to RGB (to ensure 3 channels).\n",
    "\n",
    "- If transforms are specified (e.g., resizing, tensor conversion, normalization), applies them to the image.\n",
    "- This makes sure the image tensor is in the correct format and size for your model.\n",
    "\n",
    "- Starts the caption with the special `<SOS>` (start of sentence) token.\n",
    "- Converts the caption string into a list of word indices (numerical tokens).\n",
    "- Appends the special `<EOS>` (end of sentence) token at the end.\n",
    "- This format helps your model learn when captions start and end during training.\n",
    "\n",
    "- Finally returns the transformed image tensor and the numericalized caption as a PyTorch tensor.\n",
    "- These are the inputs and targets used for training your image captioning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47394d-eafd-4f12-b7ec-40f483fc8505",
   "metadata": {},
   "source": [
    "# Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d8e500-be6c-4983-a4c0-98d77cfa5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs, caps = zip(*batch)\n",
    "    imgs = torch.stack(imgs)\n",
    "    lengths = [len(c) for c in caps]\n",
    "    caps_padded = nn.utils.rnn.pad_sequence(caps, batch_first=True, padding_value=0)\n",
    "    return imgs, caps_padded, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb91f4d-be09-4bf7-b409-35c4e746d110",
   "metadata": {},
   "source": [
    "- **Padding**: Captions have varying lengths. Neural nets require fixed-size tensors per batch, so padding is needed.\n",
    "- **Lengths**: When feeding to RNNs, the model can use the lengths to ignore padding during loss calculation or when packing sequences.\n",
    "- **Stacking images**: The images are already fixed size tensors, so stacking creates a proper batch tensor.\n",
    "\n",
    "This function prepares your batch so that images are stacked and captions are padded to the same length, while also keeping track of the original caption lengths for efficient processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ee783-3c1a-4293-998a-9c829e89ed08",
   "metadata": {},
   "source": [
    "# Model (VGG + RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828fe370-a48a-40e6-ae37-b670d4eaa941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(VGGEncoder, self).__init__()\n",
    "        vgg = models.vgg16(pretrained=True)\n",
    "        self.features = vgg.features\n",
    "        self.avgpool = vgg.avgpool\n",
    "        self.fc = nn.Linear(512*7*7, embed_size)\n",
    "        for p in self.features.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.features(images)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions[:, :-1])\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        return self.linear(hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e89e7a-acd9-4853-a142-fbcc3c4d8e69",
   "metadata": {},
   "source": [
    "The **`VGGEncoder`** Class:\n",
    "- The **Constructor**:\n",
    "   - Loads VGG16, pretrained on ImageNet.\n",
    "   - Keeps only the feature extraction part (the convolutional layers and pooling), which outputs feature maps.\n",
    "   - Adds a fully connected (fc) layer to convert the output feature maps into an embedding vector of size `embed_size`.\n",
    "   - Freezes the weights of the convolutional layers (`requires_grad=False`) so training focuses only on the new fc layer and the decoder, speeding up training and preventing overfitting.\n",
    "- The **Forward Method**:\n",
    "   - Takes a batch of images of shape `(batch_size, 3, H, W)` (3 = RGB channels).\n",
    "   - Passes them through VGG’s convolutional layers to extract feature maps.\n",
    "   - Applies average pooling to get a fixed spatial size (7x7 here) regardless of input image size.\n",
    "   - Flattens the feature maps into a vector.\n",
    "   - Passes the flattened vector through the fc layer to get the final image embedding of size `(batch_size, embed_size)`.\n",
    "\n",
    "The **`DecoderRNN`** Class:\n",
    "- The **Constructor**:\n",
    "   - Creates an embedding layer that maps vocabulary tokens (integers) to dense vectors of size `embed_size`.\n",
    "   - Defines an LSTM network with:\n",
    "   - Input size = `embed_size` (word embedding size)\n",
    "   - Hidden size = `hidden_size` (controls capacity of the LSTM)\n",
    "   - Adds a linear layer to map the LSTM’s output at each time step to a distribution over the vocabulary (logits for each word).\n",
    "- The **Forward Method**:\n",
    "   - Inputs:\n",
    "      - `features`: image embeddings from encoder of shape (batch_size, embed_size)\n",
    "      - `captions`: tokenized caption sequences (batch_size, caption_length)\n",
    "   - Steps:\n",
    "      - **Embedding tokens:** `captions[:, :-1]` selects all tokens except the last one, because during training we predict the next word given previous words. Then `self.embed(...)` maps these tokens to embedding vectors `(batch_size, caption_length - 1, embed_size)`.\n",
    "      - **Concatenate image features:** The image features tensor `(batch_size, embed_size)` is reshaped to `(batch_size, 1, embed_size)` using `unsqueeze(1)`. This is concatenated at the start of the embedded captions along the sequence dimension (dim=1), so the LSTM receives image features as the first input token, followed by the embedded words. Resulting shape: `(batch_size, caption_length, embed_size)`.\n",
    "      - **LSTM processing:** Passes this concatenated sequence through the LSTM to produce hidden states for each time step. Output hiddens shape: `(batch_size, caption_length, hidden_size)`.\n",
    "      - **Linear projection:** Applies the linear layer on each hidden state to produce logits over the vocabulary (scores for each word in the vocab) for each time step. Final output shape: `(batch_size, caption_length, vocab_size)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2aad86-21a3-4e26-9529-63f9441c4ff1",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2870789a-55a0-4bb6-9b92-1cb870ce9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    dataset = FlickrDataset(IMAGE_FOLDER, CAPTION_FILE, transform)\n",
    "    len(dataset.vocab)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    encoder = VGGEncoder(EMBED_SIZE).to(DEVICE)\n",
    "    decoder = DecoderRNN(EMBED_SIZE, HIDDEN_SIZE, len(dataset.vocab)).to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    params = list(decoder.parameters()) + list(encoder.fc.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=3e-4)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for i, (imgs, captions, _) in enumerate(dataloader):\n",
    "            imgs, captions = imgs.to(DEVICE), captions.to(DEVICE)\n",
    "            features = encoder(imgs)\n",
    "            inputs = captions[:, :-1]  # remove last token (usually <EOS>)\n",
    "            targets = captions[:, 1:]  # remove first token (<SOS>)\n",
    "            # outputs = decoder(features, captions)\n",
    "            outputs = decoder(features, captions[:, :-1])\n",
    "            # print(f\"outputs shape: {outputs.shape}\")\n",
    "            # print(f\"captions[:, 1:] shape: {captions[:, 1:].shape}\")\n",
    "            # loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions[:, 1:].reshape(-1))\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), targets.reshape(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    torch.save({'encoder': encoder.state_dict(), 'decoder': decoder.state_dict(), 'vocab': dataset.vocab}, 'caption_model.pth')\n",
    "    print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7edb3-a0d1-4890-a98f-426eb3c16f1b",
   "metadata": {},
   "source": [
    "**Data Transform and Dataset:**\n",
    "- Resizes all input images to 224×224 (VGG16 requires this input size).\n",
    "- Converts PIL images to PyTorch tensors.\n",
    "- Creates an instance of the custom `FlickrDataset`, passing image folder and caption file paths.\n",
    "- Applies the defined image transforms.\n",
    "- `dataset.vocab` holds the vocabulary (word → index mapping).\n",
    "- Loads data in batches. `collate_fn` ensures that images are stacked and captions are padded properly.\n",
    "\n",
    "**Model Initialization:**\n",
    "- `encoder` is a pretrained VGG16 model with an extra FC layer to map image features to an embedding of size EMBED_SIZE.\n",
    "- `decoder` is an LSTM that will generate captions word-by-word.\n",
    "- `len(dataset.vocab)` gives the vocabulary size to define the output dimension of the final layer in the decoder.\n",
    "\n",
    "**Loss Function and Optimizer:**\n",
    "- Cross-entropy loss is used for classification at each time step.\n",
    "- `ignore_index=0`: assuming 0 is the padding index (`<PAD>` token), this tells the loss to ignore padding when computing gradients.\n",
    "- We only want to train:\n",
    "   - All decoder parameters\n",
    "   - The encoder’s final fc layer\n",
    "- The rest of the encoder (VGG16 features) is frozen.\n",
    "- The Adam optimizer is used with learning rate `0.0003`.\n",
    "\n",
    "**Training Loop:**\n",
    "- `imgs` are image tensors, captions are padded caption sequences.\n",
    "- `_` is typically the lengths list from the collate_fn, which you are not using here but could be useful.\n",
    "- Sends data to GPU (if available).\n",
    "- Encode the batch of images to feature vectors: `(batch_size, embed_size)`\n",
    "- For teacher forcing:\n",
    "   - `Inputs`: all words except the last one (`<EOS>` removed)\n",
    "   - `Targets`: all words except the first one (`<SOS>` removed)\n",
    "- This allows the decoder to predict the next word based on previous ground truth words and image features.\n",
    "- Outputs shape: `(batch_size, sequence_length, vocab_size)`\n",
    "- Reshape outputs and targets to match the required format for `CrossEntropyLoss`.\n",
    "   - Flattened to: `(batch_size × sequence_length, vocab_size)` and `(batch_size × sequence_length,)`\n",
    "- Typical PyTorch training step:\n",
    "  - Zero gradients;  Backpropagate loss;  Update model weights\n",
    "- Saves the model's state dictionaries and vocabulary so that you can load them later for inference or fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28aed7-0002-4c43-b304-9a6d0fa70b0c",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c73a93f-90e6-46f9-a049-62995202790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Sample caption lines:\n",
      "'1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\\n'\n",
      "'1000268201_693b08cb0e.jpg,A girl going into a wooden building .\\n'\n",
      "'1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .\\n'\n",
      "'1000268201_693b08cb0e.jpg,A little girl climbing the stairs to her playhouse .\\n'\n",
      "'1000268201_693b08cb0e.jpg,A little girl in a pink dress going into a wooden cabin .\\n'\n",
      "Loaded 40455 image-caption pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sayed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sayed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [0], Loss: 8.0057\n",
      "Epoch [1/2], Step [100], Loss: 4.6346\n",
      "Epoch [1/2], Step [200], Loss: 4.4029\n",
      "Epoch [1/2], Step [300], Loss: 4.3448\n",
      "Epoch [1/2], Step [400], Loss: 4.1174\n",
      "Epoch [1/2], Step [500], Loss: 4.1776\n",
      "Epoch [1/2], Step [600], Loss: 4.0062\n",
      "Epoch [1/2], Step [700], Loss: 4.0764\n",
      "Epoch [1/2], Step [800], Loss: 4.1217\n",
      "Epoch [1/2], Step [900], Loss: 3.8847\n",
      "Epoch [1/2], Step [1000], Loss: 3.9976\n",
      "Epoch [1/2], Step [1100], Loss: 3.7505\n",
      "Epoch [1/2], Step [1200], Loss: 3.7762\n",
      "Epoch [2/2], Step [0], Loss: 3.4763\n",
      "Epoch [2/2], Step [100], Loss: 3.6393\n",
      "Epoch [2/2], Step [200], Loss: 3.7172\n",
      "Epoch [2/2], Step [300], Loss: 3.7313\n",
      "Epoch [2/2], Step [400], Loss: 3.5545\n",
      "Epoch [2/2], Step [500], Loss: 3.6710\n",
      "Epoch [2/2], Step [600], Loss: 3.7115\n",
      "Epoch [2/2], Step [700], Loss: 3.8100\n",
      "Epoch [2/2], Step [800], Loss: 3.5454\n",
      "Epoch [2/2], Step [900], Loss: 3.4751\n",
      "Epoch [2/2], Step [1000], Loss: 3.4354\n",
      "Epoch [2/2], Step [1100], Loss: 3.2445\n",
      "Epoch [2/2], Step [1200], Loss: 3.6616\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    nltk.download('punkt')\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf53a74-1cf7-446e-bdf6-87a32dc1749a",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33ce57b3-bcf6-4cee-8119-f77606bc3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path, encoder, decoder, vocab, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(DEVICE)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        feature = encoder(image)\n",
    "        caption = [vocab.stoi[\"<SOS>\"]]\n",
    "        for _ in range(MAX_LEN):\n",
    "            cap = torch.tensor([caption]).to(DEVICE)\n",
    "            output = decoder(feature, cap)\n",
    "            _, predicted = output[:, -1, :].max(1)\n",
    "            pred_id = predicted.item()\n",
    "            caption.append(pred_id)\n",
    "            if pred_id == vocab.stoi[\"<EOS>\"]:\n",
    "                break\n",
    "        caption_words = [vocab.itos[idx] for idx in caption[1:-1]]\n",
    "        return ' '.join(caption_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b3e8456-d0ab-4aa0-a5ca-c292e9365824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sayed\\AppData\\Local\\Temp\\ipykernel_36776\\1081666145.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('caption_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: a man in a blue shirt is a a . .\n"
     ]
    }
   ],
   "source": [
    "# Test on personal image\n",
    "checkpoint = torch.load('caption_model.pth')\n",
    "encoder = VGGEncoder(EMBED_SIZE).to(DEVICE)\n",
    "decoder = DecoderRNN(EMBED_SIZE, HIDDEN_SIZE, len(checkpoint['vocab'])).to(DEVICE)\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "vocab = checkpoint['vocab']\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),])\n",
    "\n",
    "test_img = 'arcane.png'\n",
    "print(\"Caption:\", generate_caption(test_img, encoder, decoder, vocab, transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acbcac9-f71c-4fb2-ba43-d6bf6b228acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
